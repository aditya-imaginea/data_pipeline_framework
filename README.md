# Data Pipeline Framework

This project enables users to build and execute custom data pipelines with stepwise scripts using a web-based form interface. It supports uploading dataset files, defining pipeline steps (with optional pre-hook and post-hook scripts), and processing data in batches using Docker containers.

---

## Features

* ğŸ“„ Upload datasets (JSON format)
* ğŸ§© Define pipeline steps with:

  * **Main Transformation Script** (required)
  * **Pre-Hook Script** (optional)
  * **Post-Hook Script** (optional)
* ğŸ”„ Batch-based processing
* ğŸ³ Dockerized execution for each batch
* ğŸ“¥ Aggregated results stored per request ID
* ğŸ§  Compatible with OpenAI API hooks

---

## Project Structure

```
project-root/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py               # FastAPI backend
â”‚   â”œâ”€â”€ templates/index.html  # HTML UI
â”‚   â”œâ”€â”€ static/script.js      # Form logic and submission
â”‚   â””â”€â”€ pipeline/
â”‚       â”œâ”€â”€ engine.py         # Executes a pipeline on a dataset
â”‚       â”œâ”€â”€ executor.py       # Runs pipeline logic stepwise
â”‚       â”œâ”€â”€ loader.py         # Loads pipeline definition and dataset
â”œâ”€â”€ batch_runner.py           # Splits dataset and runs pipeline per batch in Docker
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Setup Instructions

### Prerequisites

* Python 3.8+
* Docker
* Node.js (optional, if using build tools for frontend)

### Installation

```bash
git clone <repo-url>
cd data_pipeline_framework
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Build Docker Image

```bash
docker build -t data-pipeline-framework .
```

> This builds the Docker image used to run pipeline batches in isolated containers.

### Running the App

```bash
uvicorn app.main:app --reload
```

Then navigate to `http://localhost:8000` to use the form.

---

## Using the Form Interface

1. Fill out the pipeline form:

   * Upload your dataset (JSON file)
   * Add steps (Main script required, Pre/Post optional)
   * Specify batch size (e.g., 50)
2. Click **Submit** to send the pipeline definition and files.
3. Backend:

   * Saves scripts & files under a request UUID
   * Runs `batch_runner.py` in background using Docker per batch
   * Aggregates outputs

---

## File Upload Format

* **Dataset**: JSON list of objects
* **Pipeline JSON** (auto-generated from UI):

```json
{
  "name": "example_pipeline",
  "steps": [
    {
      "name": "validation_step",
      "pre_script": "pre_hook.py",
      "main_script": "main_transform.py",
      "post_script": "post_hook.py"
    }
  ]
}
```

---

## Developer Notes

### Submit Endpoint (`/submit_pipeline`)

* Accepts a `FormData` payload:

  * `pipeline`: JSON file
  * `dataset`: JSON file
  * `batch_size`: Integer
  * Script files with keys like `main_0`, `pre_0`, `post_0`

### Batch Runner

* Splits dataset into batch files
* Calls `engine.py` inside a Docker container:

```bash
docker run --rm -v "$PWD:/app" data-pipeline:latest \
  python pipeline/engine.py pipeline.json batch.json output.json scripts/
```

### Engine Script

* Loads the pipeline
* Applies step logic using the `PipelineExecutor`
* Saves a `state_table` JSON for each batch

---

## TODO / Enhancements

* Progress tracking UI
* Auth for upload endpoint
* Validation for uploaded scripts
* Support for multiple pipeline templates

---

## License

MIT License

---

## Author

Built by \[Aditya Palagummi] â€“ PRs welcome!
