# ğŸ› ï¸ Data Pipeline Framework

This project is a flexible and extensible data pipeline framework designed to support batch processing of data using pre-hooks, main transformations, and post-hooks. It supports concurrent execution using Docker and includes OpenAI-based validation for intelligent transformation steps.

---

## ğŸš€ Features

- ğŸ”„ Modular pipeline with configurable `pre`, `main`, and `post` hooks
- ğŸ“¦ Batch processing: processes data in 100-record chunks
- ğŸ³ Docker-based isolation for parallel execution
- ğŸ¤– OpenAI integration for semantic validation
- âœ… Unit and integration test support
- ğŸ“ Simple JSON-based pipeline definitions and datasets

---

## ğŸ“‚ Project Structure

```
data_pipeline_framework/
â”‚
â”œâ”€â”€ pipeline/                      # Core pipeline engine
â”‚   â”œâ”€â”€ engine.py                  # Main execution logic
â”‚   â”œâ”€â”€ executor.py                # Executes steps for each record
â”‚   â”œâ”€â”€ loader.py                  # Loads datasets and pipeline definitions
â”‚   â””â”€â”€ state_tracker.py          # Logs transformation states
â”‚
â”œâ”€â”€ scripts/                       # Scripts used in pipeline steps
â”‚   â””â”€â”€ sample_pipeline/
â”‚       â”œâ”€â”€ capital_pre_hook.py
â”‚       â”œâ”€â”€ capital_transform.py
â”‚       â””â”€â”€ capital_post_hook.py
â”‚
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ sample_dataset.json       # Input dataset with correct/incorrect records
â”‚
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ pipeline_definitions/
â”‚       â””â”€â”€ pipeline_definition.json
â”‚
â”œâ”€â”€ state_logs/                   # Output from pipeline steps
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_pipeline_execution.py
â”‚
â”œâ”€â”€ batch_runner.py               # Orchestrates batch-wise processing in Docker
â”œâ”€â”€ Dockerfile                    # Builds image for processing batches
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸ§ª Running Tests

To run tests using `pytest`:

```bash
pytest tests/
```

---

## ğŸ³ Docker Usage

### Build Docker Image

```bash
docker build -t data-pipeline:latest .
```

### Run Batch Processing

```bash
python batch_runner.py
```

Each batch spawns a separate Docker container, and the logs are stored in `state_logs/`.

---

## ğŸ”‘ Environment Variables

Set the OpenAI API key before running:

### Locally:
```bash
export OPENAI_API_KEY=your_openai_key
```

### In Docker (e.g. in `batch_runner.py`):

Add to the `cmd` block:
```python
"-e", "OPENAI_API_KEY"
```

---

## ğŸ§  OpenAI Integration

The framework uses OpenAI for:

- Validating the factual accuracy of input data (`capital_transform.py`)
- Assessing correctness of transformations (`capital_post_hook.py`)

Ensure you have proper access to a valid OpenAI model (`gpt-3.5-turbo`, `gpt-4`, etc.).

---

## ğŸ“ Sample Dataset Format

```json
[
  {"id": "100", "country": "India", "capital": "Delhi"},
  {"id": "101", "country": "UK", "capital": "Islamabad"} // Incorrect capital
]
```

---

## ğŸ”§ Pipeline Definition

```json
{
  "steps": [
    {
      "name": "capital_step",
      "pre_script": "scripts/sample_pipeline/capital_pre_hook.py",
      "main_script": "scripts/sample_pipeline/capital_transform.py",
      "post_script": "scripts/sample_pipeline/capital_post_hook.py"
    }
  ]
}
```

---

## ğŸ“Œ Notes

- Ensure all scripts have a `transform(record)` function.
- Output is logged per batch in the `state_logs/` folder.
- Update `requirements.txt` with necessary packages like `openai`.

---

## ğŸ“„ License

MIT License. See `LICENSE` file.
