# ğŸ§  Data Pipeline Framework (Dockerized + AI-Validated)

A modular, Dockerized data pipeline framework that allows users to define, submit, and validate data pipelines using OpenAI for transformation accuracy. Includes REST API for dynamic pipeline submissions.

---

## ğŸš€ Features

- âœ… Modular pre-hook, transform, and post-hook logic
- ğŸ³ Docker-based batch processing (parallel per 100 records)
- ğŸ§¾ OpenAI integration for data validation
- ğŸ› ï¸ REST API (FastAPI) to submit and fetch pipeline results
- ğŸ“‚ Auto-generated Swagger docs at `/docs`
- ğŸ“¦ Git-friendly structure with test coverage

---

## ğŸ“ Project Structure

```
data_pipeline_framework/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py                # FastAPI entrypoint
â”œâ”€â”€ batches/                   # Auto-generated data batches
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ engine.py              # Core pipeline execution
â”‚   â””â”€â”€ loader.py              # Loads pipeline definitions & datasets
â”œâ”€â”€ scripts/sample_pipeline/
â”‚   â”œâ”€â”€ capital_pre_hook.py
â”‚   â”œâ”€â”€ capital_transform.py
â”‚   â””â”€â”€ capital_post_hook.py
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ sample_dataset.json
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ pipeline_definitions/
â”‚       â””â”€â”€ pipeline_definition.json
â”œâ”€â”€ output/
â”‚   â””â”€â”€ aggregated_results/    # Final results per request_id
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ batch_runner.py
â””â”€â”€ README.md
```

---

## ğŸ§ª Example Record Format

### âœ… Valid
```json
{"id": "100", "country": "India", "capital": "Delhi"}
```

### âŒ Invalid
```json
{"id": "101", "country": "UK", "capital": "Islamabad"}
```

---

## âš™ï¸ Setup

### â–¶ï¸ Local Setup

```bash
# 1. Clone the repo
git clone https://github.com/your-username/data_pipeline_framework.git
cd data_pipeline_framework

# 2. Set up virtualenv
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# 3. Run API
uvicorn api.main:app --reload
```

### ğŸ³ Docker Setup

```bash
# Build the Docker image
docker build -t data-pipeline:latest .

# Run pipeline using Docker
python batch_runner.py
```

---

## ğŸ”‘ Environment Variables

Set your OpenAI key locally or via Docker:

### Local
```bash
export OPENAI_API_KEY=your-key
```

### Docker
Pass as environment variable:
```bash
docker run -e OPENAI_API_KEY=your-key ...
```

---

## ğŸŒ API Endpoints

### ğŸ“¤ `POST /submit_pipeline`

Submit your dataset and pipeline for processing.

- `pipeline_file`: JSON file
- `dataset_file`: JSON file

âœ… Response:
```json
{ "request_id": "abc123" }
```

---

### ğŸ“¥ `GET /get_result/{request_id}`

Returns the aggregated result for a given request ID.

âœ… Response:
```json
{
  "results": [
    { "id": "100", "result": "valid work" },
    ...
  ]
}
```

---

## ğŸ§  Powered By

- [FastAPI](https://fastapi.tiangolo.com/)
- [Docker](https://www.docker.com/)
- [OpenAI Python SDK](https://github.com/openai/openai-python)
- [Uvicorn](https://www.uvicorn.org/)

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Submit a PR ğŸ™Œ

---

## ğŸ“„ License

MIT License â€” feel free to use with attribution.

---

## ğŸ“¬ Questions?

Ping `aditya.palagummi@yourdomain.com` or open an issue.
